fp_vec <- numeric(length = ncol(truth_matrix))
one <- "1"
zero <- "0"
for (i in 1:ncol(truth_matrix)){
truth_calc <- truth_matrix[, i]
truth_calc <- factor(truth_calc, levels = c(0, 1))
prediction_calc <- prediction_matrix[, i]
prediction_calc <- factor(prediction_calc, levels = c(0, 1))
conf_matrix <- as.matrix(table(truth_calc,
prediction_calc))
tn <- conf_matrix[zero, zero]
tp <- conf_matrix[one, one]
fn <- conf_matrix[one, zero]
fp <- conf_matrix[zero, one]
support <- tp + fn
tn_vec[i] <- tn
tp_vec[i] <- tp
fn_vec[i] <- fn
fp_vec[i] <- fp
if(tp+fn == 0){
recall <- 0
warning('sum of true positives and false negatives is 0')
}else{
recall <- tp / (tp+fn)
}
if(tp+fp == 0){
precision <- 0
warning('sum of true and false positives is 0')
}else{
precision <- tp / (tp + fp)
}
if(isTRUE(precision == 0 & recall == 0)) {
f_score <- 0
}else{
f_score <- (2 * precision * recall)/(precision + recall)
}
macro_f_vec[i] <- f_score
recall_vec[i] <- recall
precision_vec[i] <- precision
support_vec[i] <- support
}
macro_f <- mean(macro_f_vec)
# weighted F score is a weighted average of all categories
weighted_f <- weighted.mean(macro_f_vec, support_vec)
# Alternative macro F score (not recommended)
# Reference: https://arxiv.org/pdf/1911.03347.pdf
# This is the "F1 of averages"
alternative_macro_f <- (2 * mean(precision_vec) * mean(recall_vec))/(mean(precision_vec) + mean(recall_vec))
# Micro F score calculates recall and precision over
# all labels and only then outputs the F score
recall_total <- sum(tp_vec) / (sum(tp_vec)+sum(fn_vec))
precision_total <- sum(tp_vec) / (sum(tp_vec)+sum(fp_vec))
micro_f <- (2 * precision_total * recall_total)/(precision_total + recall_total)
# accuracy
accuracy_total <- length(which(truth == prediction))/length(truth)
measures <- cbind.data.frame(macro_f,
alternative_macro_f,
weighted_f,
micro_f,
stringsAsFactors = FALSE)
colnames(measures) <- c("macro_f_score",
"f_score_of_averages",
"weighted_f_score",
"micro_f_score_acc")
return(measures)
}
truth <- c(0, 1, 2, 0, 1, 2)
#'
prediction <- c(0, 2, 1, 0, 0, 1)
#'
performance_scores_multiclass(truth, prediction)
HEMDAG::F.measure.multilabel(truth, prediction)
oxygen2::roxygenise()
roxygen2::roxygenise()
rm(list = c("performance_scores_multiclass", "performance_scores_multilabel"))
roxygen2::roxygenise()
??performance_scores_multiclass
?performance_scores_multiclass
roxygen2::roxygenise()
??performance_scores_multiclass
??performance_scores_multilabel
?performance_scores_multilabel
roxygen2::roxygenise()
?performance_scores_multilabel
roxygen2::roxygenise()
?performance_scores_multilabel
roxygen2::roxygenise()
?performance_scores_multilabel
roxygen2::roxygenise()
?performance_scores_multilabel
?performance_scores_multiclass
performance_scores_multiclass(truth, prediction)
performance_scores_multiclass <- function(truth,
prediction){
n_classes_truth <- length(unique(truth))
n_classes_pred <- length(unique(prediction))
n_classes <- max(n_classes_truth, n_classes_pred)
if (n_classes_truth == n_classes_pred | n_classes_truth > n_classes_pred){
all_classes <- sort(unique(truth))
} else {
all_classes <- sort(unique(prediction)) }
if(class(truth) != "numeric"){
stop("true labels must be supplied in a form of a numeric vector")}
if(class(prediction) != "numeric"){
stop("predicted labels must be supplied in a form of a numeric vector")}
if(!identical(length(truth), length(prediction))){
stop("true and predicted vectors must have the same dimensions")}
if(length(truth) < 2 | length(prediction) < 2){
stop("please supply a vector of truth/prediction values longer than 1")
}
truth_matrix <- matrix(nrow = length(truth), ncol = n_classes)
prediction_matrix <- matrix(nrow = length(prediction), ncol = n_classes)
for (i in 1:ncol(truth_matrix)){
current_class <- all_classes[i]
truth_matrix[, i] <- ifelse(truth == current_class, 1, 0)
prediction_matrix[, i] <- ifelse(prediction == current_class, 1, 0)
}
macro_f_vec <- numeric(length = ncol(truth_matrix))
support_vec <- numeric(length = ncol(truth_matrix))
recall_vec <- numeric(length = ncol(truth_matrix))
precision_vec <- numeric(length = ncol(truth_matrix))
tn_vec <- numeric(length = ncol(truth_matrix))
tp_vec <- numeric(length = ncol(truth_matrix))
fn_vec <- numeric(length = ncol(truth_matrix))
fp_vec <- numeric(length = ncol(truth_matrix))
one <- "1"
zero <- "0"
for (i in 1:ncol(truth_matrix)){
truth_calc <- truth_matrix[, i]
truth_calc <- factor(truth_calc, levels = c(0, 1))
prediction_calc <- prediction_matrix[, i]
prediction_calc <- factor(prediction_calc, levels = c(0, 1))
conf_matrix <- as.matrix(table(truth_calc,
prediction_calc))
tn <- conf_matrix[zero, zero]
tp <- conf_matrix[one, one]
fn <- conf_matrix[one, zero]
fp <- conf_matrix[zero, one]
support <- tp + fn
tn_vec[i] <- tn
tp_vec[i] <- tp
fn_vec[i] <- fn
fp_vec[i] <- fp
if(tp+fn == 0){
recall <- 0
warning('sum of true positives and false negatives is 0')
}else{
recall <- tp / (tp+fn)
}
if(tp+fp == 0){
precision <- 0
warning('sum of true and false positives is 0')
}else{
precision <- tp / (tp + fp)
}
if(isTRUE(precision == 0 & recall == 0)) {
f_score <- 0
}else{
f_score <- (2 * precision * recall)/(precision + recall)
}
macro_f_vec[i] <- f_score
recall_vec[i] <- recall
precision_vec[i] <- precision
support_vec[i] <- support
}
macro_f <- mean(macro_f_vec)
# weighted F score is a weighted average of all categories
weighted_f <- weighted.mean(macro_f_vec, support_vec)
# Micro F score calculates recall and precision over
# all labels and only then outputs the F score
recall_total <- sum(tp_vec) / (sum(tp_vec)+sum(fn_vec))
precision_total <- sum(tp_vec) / (sum(tp_vec)+sum(fp_vec))
micro_f <- (2 * precision_total * recall_total)/(precision_total + recall_total)
# accuracy
accuracy_total <- length(which(truth == prediction))/length(truth)
measures <- cbind.data.frame(macro_f,
weighted_f,
micro_f,
stringsAsFactors = FALSE)
colnames(measures) <- c("macro_f_score",
"f_score_of_averages",
"weighted_f_score",
"micro_f_score_acc")
return(measures)
}
performance_scores_multiclass(truth, prediction)
performance_scores_multiclass <- function(truth,
prediction){
n_classes_truth <- length(unique(truth))
n_classes_pred <- length(unique(prediction))
n_classes <- max(n_classes_truth, n_classes_pred)
if (n_classes_truth == n_classes_pred | n_classes_truth > n_classes_pred){
all_classes <- sort(unique(truth))
} else {
all_classes <- sort(unique(prediction)) }
if(class(truth) != "numeric"){
stop("true labels must be supplied in a form of a numeric vector")}
if(class(prediction) != "numeric"){
stop("predicted labels must be supplied in a form of a numeric vector")}
if(!identical(length(truth), length(prediction))){
stop("true and predicted vectors must have the same dimensions")}
if(length(truth) < 2 | length(prediction) < 2){
stop("please supply a vector of truth/prediction values longer than 1")
}
truth_matrix <- matrix(nrow = length(truth), ncol = n_classes)
prediction_matrix <- matrix(nrow = length(prediction), ncol = n_classes)
for (i in 1:ncol(truth_matrix)){
current_class <- all_classes[i]
truth_matrix[, i] <- ifelse(truth == current_class, 1, 0)
prediction_matrix[, i] <- ifelse(prediction == current_class, 1, 0)
}
macro_f_vec <- numeric(length = ncol(truth_matrix))
support_vec <- numeric(length = ncol(truth_matrix))
recall_vec <- numeric(length = ncol(truth_matrix))
precision_vec <- numeric(length = ncol(truth_matrix))
tn_vec <- numeric(length = ncol(truth_matrix))
tp_vec <- numeric(length = ncol(truth_matrix))
fn_vec <- numeric(length = ncol(truth_matrix))
fp_vec <- numeric(length = ncol(truth_matrix))
one <- "1"
zero <- "0"
for (i in 1:ncol(truth_matrix)){
truth_calc <- truth_matrix[, i]
truth_calc <- factor(truth_calc, levels = c(0, 1))
prediction_calc <- prediction_matrix[, i]
prediction_calc <- factor(prediction_calc, levels = c(0, 1))
conf_matrix <- as.matrix(table(truth_calc,
prediction_calc))
tn <- conf_matrix[zero, zero]
tp <- conf_matrix[one, one]
fn <- conf_matrix[one, zero]
fp <- conf_matrix[zero, one]
support <- tp + fn
tn_vec[i] <- tn
tp_vec[i] <- tp
fn_vec[i] <- fn
fp_vec[i] <- fp
if(tp+fn == 0){
recall <- 0
warning('sum of true positives and false negatives is 0')
}else{
recall <- tp / (tp+fn)
}
if(tp+fp == 0){
precision <- 0
warning('sum of true and false positives is 0')
}else{
precision <- tp / (tp + fp)
}
if(isTRUE(precision == 0 & recall == 0)) {
f_score <- 0
}else{
f_score <- (2 * precision * recall)/(precision + recall)
}
macro_f_vec[i] <- f_score
recall_vec[i] <- recall
precision_vec[i] <- precision
support_vec[i] <- support
}
macro_f <- mean(macro_f_vec)
# weighted F score is a weighted average of all categories
weighted_f <- weighted.mean(macro_f_vec, support_vec)
# Micro F score calculates recall and precision over
# all labels and only then outputs the F score
recall_total <- sum(tp_vec) / (sum(tp_vec)+sum(fn_vec))
precision_total <- sum(tp_vec) / (sum(tp_vec)+sum(fp_vec))
micro_f <- (2 * precision_total * recall_total)/(precision_total + recall_total)
# accuracy
accuracy_total <- length(which(truth == prediction))/length(truth)
measures <- cbind.data.frame(macro_f,
weighted_f,
micro_f,
stringsAsFactors = FALSE)
colnames(measures) <- c("macro_f_score",
"weighted_f_score",
"micro_f_score_acc")
return(measures)
}
performance_scores_multiclass(truth, prediction)
roxygen2::roxygenise()
rm(list = c("performance_scores_multiclass")
)
roxygen2::roxygenise()
?performance_scores_multiclass
?performance_scores_multilabel
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
yardstick::f_meas_vec(truth, prediction)
yardstick::f_meas_vec(as.factor(truth), as.factor(prediction))
yardstick::f_meas_vec(as.factor(truth), as.factor(prediction), estimator = "macro")
yardstick::f_meas_vec(as.factor(truth), as.factor(prediction), estimator = "macro_weighted")
yardstick::f_meas_vec(as.factor(truth), as.factor(prediction), estimator = "micro")
performance_scores_multiclass <- function(truth,
prediction){
n_classes_truth <- length(unique(truth))
n_classes_pred <- length(unique(prediction))
n_classes <- max(n_classes_truth, n_classes_pred)
if (n_classes_truth == n_classes_pred | n_classes_truth > n_classes_pred){
all_classes <- sort(unique(truth))
} else {
all_classes <- sort(unique(prediction)) }
if(class(truth) != "numeric"){
stop("true labels must be supplied in a form of a numeric vector")}
if(class(prediction) != "numeric"){
stop("predicted labels must be supplied in a form of a numeric vector")}
if(!identical(length(truth), length(prediction))){
stop("true and predicted vectors must have the same dimensions")}
if(length(truth) < 2 | length(prediction) < 2){
stop("please supply a vector of truth/prediction values longer than 1")
}
truth_matrix <- matrix(nrow = length(truth), ncol = n_classes)
prediction_matrix <- matrix(nrow = length(prediction), ncol = n_classes)
for (i in 1:ncol(truth_matrix)){
current_class <- all_classes[i]
truth_matrix[, i] <- ifelse(truth == current_class, 1, 0)
prediction_matrix[, i] <- ifelse(prediction == current_class, 1, 0)
}
macro_f_vec <- numeric(length = ncol(truth_matrix))
support_vec <- numeric(length = ncol(truth_matrix))
recall_vec <- numeric(length = ncol(truth_matrix))
precision_vec <- numeric(length = ncol(truth_matrix))
tn_vec <- numeric(length = ncol(truth_matrix))
tp_vec <- numeric(length = ncol(truth_matrix))
fn_vec <- numeric(length = ncol(truth_matrix))
fp_vec <- numeric(length = ncol(truth_matrix))
one <- "1"
zero <- "0"
for (i in 1:ncol(truth_matrix)){
truth_calc <- truth_matrix[, i]
truth_calc <- factor(truth_calc, levels = c(0, 1))
prediction_calc <- prediction_matrix[, i]
prediction_calc <- factor(prediction_calc, levels = c(0, 1))
conf_matrix <- as.matrix(table(truth_calc,
prediction_calc))
tn <- conf_matrix[zero, zero]
tp <- conf_matrix[one, one]
fn <- conf_matrix[one, zero]
fp <- conf_matrix[zero, one]
support <- tp + fn
tn_vec[i] <- tn
tp_vec[i] <- tp
fn_vec[i] <- fn
fp_vec[i] <- fp
if(tp+fn == 0){
recall <- 0
warning('sum of true positives and false negatives is 0')
}else{
recall <- tp / (tp+fn)
}
if(tp+fp == 0){
precision <- 0
warning('sum of true and false positives is 0')
}else{
precision <- tp / (tp + fp)
}
if(isTRUE(precision == 0 & recall == 0)) {
f_score <- 0
}else{
f_score <- (2 * precision * recall)/(precision + recall)
}
macro_f_vec[i] <- f_score
recall_vec[i] <- recall
precision_vec[i] <- precision
support_vec[i] <- support
}
macro_f <- mean(macro_f_vec)
# weighted F score is a weighted average of all categories
weighted_f <- weighted.mean(macro_f_vec, support_vec)
# Micro F score calculates recall and precision over
# all labels and only then outputs the F score
recall_total <- sum(tp_vec) / (sum(tp_vec)+sum(fn_vec))
precision_total <- sum(tp_vec) / (sum(tp_vec)+sum(fp_vec))
micro_f <- (2 * precision_total * recall_total)/(precision_total + recall_total)
# accuracy
accuracy_total <- length(which(truth == prediction))/length(truth)
measures <- cbind.data.frame(macro_f,
weighted_f,
micro_f,
stringsAsFactors = FALSE)
colnames(measures) <- c("macro_f_score",
"weighted_f_score",
"micro_f_score_acc")
return(measures)
}
yardstick::f_meas_vec(as.factor(truth), as.factor(prediction), estimator = "micro")
yardstick::f_meas_vec(as.factor(truth), as.factor(prediction), estimator = "macro")
yardstick::f_meas_vec(as.factor(truth), as.factor(prediction), estimator = "macro_weighted")
performance_scores_multiclass(truth, prediction)
roxygen2::roxygenise()
rm(list = c("performance_scores_multiclass"))
roxygen2::roxygenise()
yardstick::accuracy_vec(as.factor(truth), as.factor(prediction))
check <- performance_scores_multiclass(truth, prediction)
View(check)
truth_vector <- c(0, 1, 2, 0, 1, 2)
prediction_vector <- c(0, 2, 1, 0, 0, 1)
performance_scores_multiclass(truth = truth_vector,
prediction = prediction_vector)
(performance_scores_multiclass(truth, prediction))
(truth)
truth_matrix <- matrix(c(0, 1, 0, 0, 1, 1, 0, 1, 1), nrow = 3, ncol = 3)
prediction_matrix <- matrix(c(0, 1, 1, 0, 1, 1, 0, 1, 0), nrow = 3, ncol = 3)
(truth_matrix)
performance_scores_multilabel(truth = truth_matrix,
prediction = prediction_matrix,
reference_category = 1,
metrics = "both")
kable(head(performances, n=1) ,"html", booktabs = T) %>%
kable_styling(bootstrap_options="striped", full_width=T)
library(dplyr)
library(kableExtra)
library(knitr)
kable(head(performances, n=1) ,"html", booktabs = T) %>%
kable_styling(bootstrap_options="striped", full_width=T)
truth_vector <- c(0, 1, 2, 0, 1, 2)
prediction_vector <- c(0, 2, 1, 0, 0, 1)
performances <- performance_scores_multiclass(truth = truth_vector,
prediction = prediction_vector)
kable(head(performances, n=1) ,"html", booktabs = T) %>%
kable_styling(bootstrap_options="striped", full_width=T)
performances <- performance_scores_multilabel(truth = truth_matrix,
prediction = prediction_matrix,
reference_category = 1,
metrics = "both")
performances <- performance_scores_multilabel(truth = truth_matrix,
prediction = prediction_matrix,
reference_category = 1,
metrics = "both")
kable(head(performances, n=1) ,"html", booktabs = T) %>%
kable_styling(bootstrap_options="striped", full_width=T)
roxygen2::roxygenise()
usethis::use_vignette("performance-measures")
devtools::build_rmd()
devtools::build_rmd("performance-measures.Rmd")
devtools::build_rmd("vignettes/performance-measures.Rmd")
devtools::build()
library(sklearnR.metrics)
devtools::build()
devtools::build()
library(sklearnR.metrics)
library(sklearnR.metrics)
devtools::build()
library(sklearnR.metrics)
install("E:/Projects/R packages/sklearnR.metrics_0.0.2.0.tar.gz")
devtools::install("E:/Projects/R packages/sklearnR.metrics_0.0.2.0.tar.gz")
devtools::install("E:/Projects/R packages/sklearnR.metrics_0.0.1.0.tar.gz")
devtools::install("E:/Projects/R packages/sklearnR.metrics")
library(sklearnR.metrics)
devtools::build()
library(sklearnR.metrics)
# devtools::install_github("amacanovic/sklearnR.metrics")
library(sklearnR.metrics)
library(sklearnR.metrics)
library(sklearnR.metrics)
truth_matrix <- matrix(c(0, 1, 0, 0, 1, 1, 0, 1, 1), nrow = 3, ncol = 3)
prediction_matrix  <-  matrix(c(0, 1, 1, 0, 1, 1, 0, 1, 0), nrow = 3, ncol = 3)
performances <- performance_scores_multilabel(truth = truth_matrix,
prediction = prediction_matrix,
reference_category = 1,
measures = "both")
??performance_scores_multilabel
performances <- sklearnR.metrics::performance_scores_multilabel(truth = truth_matrix,
prediction = prediction_matrix,
reference_category = 1,
measures = "both")
sklearnR.metrics::performance_scores_multiclass()
truth_vector <- c(0, 1, 2, 0, 1, 2)
prediction_vector <- c(0, 2, 1, 0, 0, 1)
performances  <-  performance_scores_multiclass(truth = truth_vector,
prediction = prediction_vector)
roxygen2
roxygen2::roxygenise()
roxygen2::roxygenise()
roxygen2::roxygenise()
build()
devtools::build()
performances <- performance_scores_multilabel(truth = truth_matrix,
prediction = prediction_matrix,
reference_category = 1,
measures = "both")
performances <- performance_scores_multilabel(truth = truth_matrix,
prediction = prediction_matrix,
reference_category = 1,
metrics = "both")
devtools::build()
vignette("performance-measures", sklearnR.metrics)
vignette("performance-measures", "sklearnR.metrics")
browseVignettes("sklearnR.metrics")
performances <- performance_scores_multilabel(truth = truth_matrix,
prediction = prediction_matrix,
reference_category = 1,
metrics = "both")
remotes::install_local(..., build_vignettes = TRUE)
remotes::install_local(build_vignettes = TRUE)
remotes::install_local(build_vignettes = TRUE, force = TRUE)
browseVignettes("sklearnR.metrics")
tools::buildVignettes(dir = ".", tangle=TRUE)
dir.create("inst/doc")
file.copy(dir("vignettes", full.names=TRUE), "inst/doc", overwrite=TRUE)
library(KeynessMeasures)
browseVignettes(KeynessMeasures)
browseVignettes("KeynessMeasures")
devtools::build()
devtools::build()
devtools::build()
devtools::build_readme()
devtools::build_readme()
devtools::install_github("amacanovic/sklearnR.metrics")
devtools::build_vignettes()
devtools::build_vignettes()
