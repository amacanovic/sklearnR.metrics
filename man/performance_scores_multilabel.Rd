% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/performance_scores_multilabel.R
\name{performance_scores_multilabel}
\alias{performance_scores_multilabel}
\title{Calculate the (sklearn) multi-label F-score and accuracy statistics}
\usage{
performance_scores_multilabel(
  truth,
  prediction,
  reference_category = 1,
  metrics = "both"
)
}
\arguments{
\item{truth}{a \code{matrix} with true labels, where labels are in columns
and cases are in rows; labels can only be 0 or 1}

\item{prediction}{a \code{matrix} with predicted labels, where labels are in columns
and cases are in rows; labels can only be 0 or 1}

\item{reference_category}{specifies which label to use as the 
reference to calculate the F-score measures; can take values \code{1} or \code{0}; default value is \code{1}}

\item{metrics}{specifies which metrics to output; can take values \code{"label_wise"}, \code{"observation_wise"}, or
\code{"both"}; default is \code{"both"}}
}
\value{
A data-frame with the following measures:

- Label-wise metrics: these metrics are calculated either per individual
label and then averaged; or are calculated across all labels

1. Macro F-score (average of F-scores for each label); corresponds to 
sklearn.metrics.f1_score() with parameter 'average' set to 'macro'; 
 
2. Weighted F-score (average of F-scores for each label weighted by
label frequencies); corresponds to sklearn.metrics.f1_score with parameter 'average'
set to 'weighted';

3. F-score of averages (harmonic mean of average precision and recall scores per label);

4. Micro F-score (calculated using total true positives, false negatives and false positives
across all labels); corresponds to sklearn.metrics.f1_score with parameter 'micro'
set to 'weighted';

5. Accuracy (calculated as the percentage of correctly predicted categories across all labels);


- Observation-wise metrics: these metrics are calculated per observation, and 
then averaged for the whole dataset:

6. Observation-based micro f-score (harmonic mean of precision and recall
calculated per observation); corresponds to R mlr package 
'multilabel.f1 score';

7. Average observation-based micro f-score (harmonic mean of average precision
and recall scores per observation); corresponds to F score output of 
HEMDAG R package F.measure.multilabel function with b.per.example set to 'FALSE';

8. Observation-based F-score (average of F-scores for each observation); corresponds to 
sklearn.metrics.f1_score with parameter 'average' set to 'samples' and avF 
score output of HEMDAG R package F.measure.multilabel function with b.per.example set to 'FALSE';

9. Subset accuracy (calculated as the percentage of observations where all labels were correctly predicted);
corresponds to sklearn.metrics.accuracy for multi-label problems.
}
\description{
A function to calculate different F-score and accuracy measures 
for multi-label classification problems. 
This function produces the same results as those obtained by the
python scikit package, with the addition of a few more measures.
}
\examples{
truth <- matrix(c(0, 1, 0, 0, 1, 1, 0, 1, 1), nrow = 3, ncol = 3)

prediction <- matrix(c(0, 1, 1, 0, 1, 1, 0, 1, 0), nrow = 3, ncol = 3)

performance_scores_multilabel(truth, prediction, reference_category = 1, metrics = "both")

}
